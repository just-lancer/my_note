<h1 align = "center">Hadoop_HDFS&YARN_HA部署文档
</h1>

==**说明**==

**由于没有具体的服务器，部署文档以虚拟机进行**

**受限于计算机内存，本次部署的Hadoop集群规模为3台服务器**

# 一、技术选型

-   **VMWare 16**
-   **CentOS 7**
-   **JDK 8**
-   **Hadoop-3.1.3**
-   **Zookeeper-3.5.7**
-   **Kafka-2.12-3.0.0**
    -   注意：zookeeper从3.5.5版本开始，软件包分为-bin.tar.gz包（已编译包）和.tar.gz（未编译的包），如果是未编译的包需要先进行编译之后才能使用，所以直接下载已编译包即可

# 二、集群规划（HA模式）

**JournalNode服务用于同步主备NameNode的Eidts编辑日志**

|    hadoop132    |  hadoop133  |    hadoop134    |
| :-------------: | :---------: | :-------------: |
|    Zookeeper    |  Zookeeper  |    Zookeeper    |
|   JournalNode   | JournalNode |   JournalNode   |
|    NameNode     |  NameNode   |      **—**      |
|    DataNode     |  DataNode   |    DataNode     |
| ResourceManager |    **—**    | ResourceManager |
|   NodeManager   | NodeManager |   NodeManager   |
|      Kafka      |    Kafka    |      Kafka      |

# 三、虚拟机创建

<u>**基于VMWare 16创建3台虚拟机节点的步骤，略**</u>

<u>**为节约资源，三台虚拟机均为最小化安装，其中网络模式使用NAT模式**</u>

<u>**创建好的节点需要进行的配置：（三台节点均需要进行的配置）**</u>

**<u>以下操作均使用root用户进行</u>**

-   **安装软件包：`yum install -y epel-release`**

-   **安装vim编辑器：`yum install -y vim`**

-   **安装网络工具包：`yum install -y net-tools`**

-   **安装rsync同步工具：`yum install -y rsync`**

-   **静止防火墙开启自启：`systemctl disable firewalld`**

-   **网络配置：**

    -   **静态化IP地址：`vim /etc/sysconfig/network-scripts/ifcfg-ens33`**

        ```txt
        TYPE="Ethernet"
        PROXY_METHOD="none"
        BROWSER_ONLY="no"
        BOOTPROTO="static" # IP的配置方法[none|static|bootp|dhcp]（引导时不使用协议|静态分配IP|BOOTP协议|DHCP协议），此配置项设置为static
        DEFROUTE="yes"
        IPV4_FAILURE_FATAL="no"
        IPV6INIT="yes"
        IPV6_AUTOCONF="yes"
        IPV6_DEFROUTE="yes"
        IPV6_FAILURE_FATAL="no"
        IPV6_ADDR_GEN_MODE="stable-privacy"
        NAME="ens33"
        UUID="d6e62aa8-919e-4f14-8a0a-74d6953b61fe"
        DEVICE="ens33"
        ONBOOT="yes"
        IPADDR="192.168.200.132" # IP地址，三台节点的IP地址分别设置为192.168.200.132、192.168.200.132、192.168.200.132
        GATEWAY="192.168.200.2" # 网关
        DNS1="192.168.200.2" # 域名解析器
        ```

        **需要注意的是关于IP地址的网段，一般情况下，会和Windows系统的网段相同，安装好的虚拟机能直接与Windows进行通信，不需要额外设置。如果网段不同，那么需要将VMWare的NAT模式的网段与Windows的VMnet8的IPv4的网段设置成相同的**

-   **修改三台节点的主机名：`vim /etc/hostname`**

    ```txt
    hadoop132
    ```

    ```txt
    hadoop133
    ```

    ```
    hadoop134
    ```

-   **修改主机地址映射：`vim /etc/hosts`**

    ```txt
    192.168.200.132 hadoop132
    192.168.200.133 hadoop133
    192.168.200.134 hadoop134
    ```

-   **一般不会直接拿root用户操作，所以创建一个普通用户，但是需要配置一些权限**

    -   **创建用户：`useradd justlancer`**
    -   **配置密码：`passwd justlancer`**。执行命令后，输入两次密码**
    -   **配置用户执行命令的相关权限：`vim /etc/sudoers`**![image-20230223174909162](.\大数据组件部署文档\image-20230223174909162.png)

-   **创建justlancer用户需要使用的目录：**

    -   **创建软件包存放位置：`mkdir /opt/software`**
    -   **创建解压包存放位置：`mkdir /opt/module`**
    -   **修改文件所属用户：`chown justlancer /opt/software /opt/module`**
    -   **修改文件所属用户组：`chgrp justlancer /opt/software /opt/module`**

-   **至此，三台节点需使用root用户进行的配置基本完成，重启节点：`reboot`**

>   **==以下均为使用justlancer用户进行的操作==**
>
>   **==以下均为在hadoop132节点上进行的操作，在其他节点也需要进行同样的操作==**

-   **使用justlancer用户登录服务器**

-   **配置三台节点间的ssh免密登录：**

    -   **在任意一台节点用ssh登录任意一台其他节点，用以在/home/justlancer目录下创建.ssh文件，以hadoop132登录hadoop133为例（==这个操作只需要在三台节点中，任意一台节点执行一次即可==）：`[justlancer@hadoop132 ~]$ ssh hadoop133`**

    -   **进入.ssh目录：`cd /home/justlancer/.ssh`**

    -   **生成公钥和私钥，遇到提示连续三次输出回车即可：`ssh-keygen -t rsa`**

    -   **将生成的公钥发送到其他节点（包括自己），需执行三次`ssh-copy-id`命令：**

        ```bash
        [justlancer@hadoop132 .ssh]$ ssh-copy-id hadoop132
        [justlancer@hadoop132 .ssh]$ ssh-copy-id hadoop133
        [justlancer@hadoop132 .ssh]$ ssh-copy-id hadoop134
        ```

-   **编写集群间数据同步脚本，脚本名称xsync：**

    -   **/home/justlancer中创建bin目录：`mkdir /home/justlancer/bin`**

    -   **创建脚本文件并编写脚本：`vim /home/justlancer/bin/xsync`**

        ```bash
        #!/bin/bash
        
        #1. 判断参数个数
        if [ $# -lt 1 ]
        then
            echo Not Enough Arguement!
            exit;
        fi
        
        #2. 遍历集群所有机器
        for host in hadoop132 hadoop133 hadoop134
        do
            echo ====================  $host  ====================
            #3. 遍历所有目录，挨个发送
        
            for file in $@
            do
                #4. 判断文件是否存在
                if [ -e $file ]
                    then
                        #5. 获取父目录
                        pdir=$(cd -P $(dirname $file); pwd)
        
                        #6. 获取当前文件的名称
                        fname=$(basename $file)
                        ssh $host "mkdir -p $pdir"
                        rsync -av $pdir/$fname $host:$pdir
                    else
                        echo $file does not exists!
                fi
            done
        done
        ```
        
    -   **赋予脚本执行权限：`chmod u=rwx xsync`**
    
    -   **分发脚本自己：`xsync xsync`**

**==至此，集群安装部署的准备工作就完成了==**

# 四、安装Java运行环境

**==以下操作均在hadoop132节点上操作，完成之后进行文件分发==**

-   **利用xftp工具，将JDK 8安装包上传到`/opt/software`中**

-   **解压到/opt/module目录中：`tar -zxvf /opt/software/jdk-8u212-linux-x64.tar.gz -C /opt/moudule/`**

-   **配置环境变量：**

    -   **在/etc/profile.d目录下创建.sh文件，用于存放自己的环境变量：`vim /etc/profile.d/my_env.sh`**

        ```txt
        # JAVA_HOME
        export JAVA_HOME=/opt/module/jdk1.8.0_333
        export PATH=$PATH:$JAVA_HOME/bin
        ```

    -   **执行文件，让环境变量生效：`source /etc/profile`**

    -   **检查环境变量是否生效，如果打印jdk信息，则说明环境变量生效：`java -version`**

-   **编写查看各节点Java进程的脚本：vim /home/justlancer/jpsall**

    ```bash
    #!/bin/bash
    for machineIP in hadoop132 hadoop133 hadoop134; do
        echo "============== $machineIP ================="
        ssh $machineIP jps
    done
    ```

-   **赋予脚本执行权限：`chmod u=rwx /home/justlancer/jpsall`**

-   **分发文件：**

    -   **JDK：`xsync /opt/module/jdk1.8.0_212/`**
    -   **/etc/profile.d/my_env.sh 该文件位于etc目录下，使用justlancer用于分发文件，会存在问题，直接手动在各节点配置Java环境变量**

# 五、安装并配置Zookeeper集群

**==以下操作均在hadoop132节点上操作，完成之后进行文件分发。区别于其他集群，Zookeeper集群中myid配置文件是用于配置Zookeeper节点的身份，每个Zookeeper节点的配置内容是不同的==**

-   **利用xftp工具，将apache-zookeeper-3.5.7-bin.tar.gz上传到/opt/software中**

-   **解压到/opt/module目录中：tar -zxvf /opt/software/apache-zookeeper-3.5.7-bin.tar.gz -C /opt/module/**

-   **对Zookeeper解压包进行重命名，去掉apache前缀：`mv /opt/module/apache-zookeeper-3.5.7-bin/ /opt/module/zookeeper-3.5.7-bin/`**

-   **配置环境变量：`vim /etc/profile.d/my_env.sh`**

    ```txt
    #ZOOKEEPER_HOME
    export ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7-bin
    export PATH=$PATH:$ZOOKEEPER_HOME/bin
    ```

-   **执行文件，让环境变量生效：`source /etc/profile`**

-   **配置Zookeeper集群**

    -   **在/opt/module/apache-zookeeper-3.5.7-bin/目录下创建zkData目录，用于存放Zookeeper产生的数据：`mkdir /opt/module/zookeeper-3.5.7-bin/zkData`**

    -   **在zkData目录下创建myid文件，用于表示Zookeeper节点身份：`vim /opt/module/zookeeper-3.5.7-bin/zkData/myid`**

        ```txt
        2
        ```

        **==表示hadoop132节点上的zookeeper节点的身份为2==**

    -   **分发myid文件：`xsync /opt/module/zookeeper-3.5.7-bin/zkData/myid`**

    -   **修改hadoop133、hadoop134节点的myid**

        -   **hadoop133：`vim /opt/module/zookeeper-3.5.7-bin/zkData/myid`**

            ```txt
            3
            ```

        -   **hadoop134：`vim /opt/module/zookeeper-3.5.7-bin/zkData/myid`**

            ```txt
            4
            ```

    -   **重命名zoo_sample.cfg文件，并进行配置和分发**

        -   **重命名：`mv zoo_sample.cfg zoo.cfg`**
        -   **修改并添加配置：**![image-20230224120615370](.\大数据组件部署文档\image-20230224120615370.png)
        -   **分发配置的文件，并去hadoop133、hadoop134节点删除zoo_sample.cfg文件**
            -   **分发zoo.cfg：`xsync /opt/module/zookeeper-3.5.7-bin/conf/zoo.cfg`**
            -   **在hadoop133、hadoop134节点上，删除zoo_sample.cfg：`rm /opt/module/zookeeper-3.5.7-bin/conf/zoo_sample.cfg`**

-   **编写Zookeeper集群群起群停脚本：`vim /home/justlancer/bin/zk_mine.sh`**

    ```bash
    #!/bin/bash
    # 群起群停zookeeper集群
    if [ $# -lt 1 ]; then
        echo '脚本缺少参数'
        exit;
    fi ;
    
    case $1 in
        'start' )
                for machineIP in hadoop132 hadoop133 hadoop134; do
                    echo "====================启动$machineIP zookeeper====================";
                    ssh $machineIP "/opt/module/zookeeper-3.5.7-bin/bin/zkServer.sh start"
                done
            ;;
        'stop' )
                for machineIP in hadoop132 hadoop133 hadoop134; do
                    echo "====================停止$machineIP zookeeper====================";
                    ssh $machineIP "/opt/module/zookeeper-3.5.7-bin/bin/zkServer.sh stop"
                done
            ;;
        * )
            echo '不是有效参数。参数仅包含<start><stop>'
            ;;
    esac
    ```

-   **赋予脚本执行权限：`chmod u=rwx /home/justlancer/bin/zk_mine.sh`**

-   **分发脚本：xsync `/home/justlancer/bin/zk_mine.sh`**

# 六、安装并配置Hadoop HA集群

**==以下操作均在hadoop132节点上操作，完成之后进行文件分发==**

-   **利用xftp工具，将Hadoop-3.1.3.tar.gz压缩包上传到/opt/software中**

-   **解压到/opt/module目录中：`tar -zxvf /opt/software/hadoop-3.1.3.tar.gz -C /opt/module/`**

-   **对hadoop解压包进行重命名，添加-ha后缀，表示为Hadoop HA所使用的Hadoop包：`mv /opt/module/hadoop-3.1.3/ /opt/module/hadoop-3.1.3-ha/`**

-   **配置环境变量：`vim /etc/profile.d/my_env.sh`**

    ```txt
    #HAOODP_HOME
    export HADOOP_HOME=/opt/module/hadoop-3.1.3-ha
    export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
    ```

-   **执行文件，让环境变量生效：`source /etc/profile`**

-   **配置HDFS HA和YARN HA，对Hadoop核心配置文件进行配置，并分发**

    -   **配置core-site.xml：`vim /opt/module/hadoop-3.1.3-ha/etc/hadoop/core-site.xml`**

        ```xml
        <!-- Hadoop HDFS高可用配置文件 -->
        <!-- core-site.xml文件配置 -->
        <configuration>
            <!-- 把多个 NameNode 的地址组装成一个集群，集群地址名称为：mycluster -->
            <property>
                <name>fs.defaultFS</name>
                <value>hdfs://mycluster</value>
            </property>
        
            <!-- 指定 Hadoop 运行时产生文件的存储目录 -->
            <property>
                <name>hadoop.tmp.dir</name>
                <value>/opt/module/hadoop-3.1.3-ha/ha-data</value>
            </property>
        
            <!-- 指定 zkfc 要连接的 zkServer 地址 -->
            <property>
                <name>ha.zookeeper.quorum</name>
                <value>hadoop132:2181,hadoop133:2181,hadoop134:2181</value>
            </property>
        
            <!-- 调整以下两个参数，以保证NameNode能顺利连接JournalNode -->
            <!-- HDFS启动脚本start-dfs.sh，默认先启动NameNode，后启动DataNode，最后启动JournalNode -->
            <!-- 如果在重试次数 * 重试间隔时间之内，NameNode还未连接上JournalNode，那么就会连接失败 -->
            <!-- NameNode 连接 JournalNode 重试次数，默认是 10 次 -->
            <property>
                <name>ipc.client.connect.max.retries</name>
                <value>20</value>
            </property>
        
            <!-- 重试时间间隔，默认 1s -->
            <property>
                <name>ipc.client.connect.retry.interval</name>
                <value>5000</value>
            </property>
        </configuration>
        ```

    -   **配置hdhf-site.xml，`vim /opt/module/hadoop-3.1.3-ha/etc/hadoop/hdfs-site.xml`**

        ```xml
        <!-- Hadoop HDFS高可用配置文件 -->
        <!-- hdfs-site.xml文件配置 -->
        <configuration>
            <!-- NameNode 数据存储目录 -->
            <property>
                <name>dfs.namenode.name.dir</name>
                <value>file://${hadoop.tmp.dir}/namenode-data</value>
            </property>
        
            <!-- DataNode 数据存储目录 -->
            <property>
                <name>dfs.datanode.data.dir</name>
                <value>file://${hadoop.tmp.dir}/datanode-data</value>
            </property>
        
            <!-- JournalNode 数据存储目录 -->
            <property>
                <name>dfs.journalnode.edits.dir</name>
                <value>${hadoop.tmp.dir}/journalnode-data</value>
            </property>
        
            <!-- HDFS集群名称 这个名称需要和core-site.xml中配置的集群名称相同 -->
            <property>
                <name>dfs.nameservices</name>
                <value>mycluster</value>
            </property>
        
            <!-- 集群中 NameNode 节点都有哪些 -->
            <property>
                <name>dfs.ha.namenodes.mycluster</name>
                <value>nn1,nn2</value>
            </property>
        
            <!-- NameNode 的 RPC 通信地址 -->
            <property>
                <name>dfs.namenode.rpc-address.mycluster.nn1</name>
                <value>hadoop132:8020</value>
            </property>
        
            <property>
                <name>dfs.namenode.rpc-address.mycluster.nn2</name>
                <value>hadoop133:8020</value>
            </property>
        
            <!-- NameNode 的 http 通信地址 -->
            <property>
                <name>dfs.namenode.http-address.mycluster.nn1</name>
                <value>hadoop132:9870</value>
            </property>
        
            <property>
                <name>dfs.namenode.http-address.mycluster.nn2</name>
                <value>hadoop133:9870</value>
            </property>
        
            <!-- 指定 NameNode 元数据在 JournalNode 上的存放位置 -->
            <property>
                <name>dfs.namenode.shared.edits.dir</name>
                <value>qjournal://hadoop132:8485;hadoop133:8485/mycluster</value>
            </property>
        
            <!-- 访问代理类： client 用于确定哪个 NameNode 为 Active -->
            <property>
                <name>dfs.client.failover.proxy.provider.mycluster</name>
                <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
            </property>
        
            <!-- 配置隔离机制，即同一时刻只能有一台服务器对外响应 -->
            <property>
                <name>dfs.ha.fencing.methods</name>
                <value>sshfence</value>
            </property>
        
            <!-- 使用隔离机制时需要 ssh 秘钥登录-->
            <property>
                <name>dfs.ha.fencing.ssh.private-key-files</name>
                <value>/home/atguigu/.ssh/id_rsa</value>
            </property>
        
            <!-- 启用 NameNode 故障自动转移 -->
            <property>
                <name>dfs.ha.automatic-failover.enabled</name>
                <value>true</value>
            </property>
        </configuration>
        ```

    -   **配置yarn-site.xml，`vim /opt/module/hadoop-3.1.3-ha/etc/hadoop/yarn-site.xml`**

        ```xml
        <!-- Hadoop YARN高可用配置 -->
        <!-- yarn-site.xml文件配置 -->
        <configuration>
            <!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可运行MapReduce程序 -->
            <!-- 还可以配置成spark_shuffle -->
            <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
            </property>
        
            <!-- 启用 ResourceManager HA -->
            <property>
                <name>yarn.resourcemanager.ha.enabled</name>
                <value>true</value>
            </property>
        
            <!-- 声明YARN集群的集群名称 -->
            <property>
                <name>yarn.resourcemanager.cluster-id</name>
                <value>cluster-yarn</value>
            </property>
        
            <!-- 声明YARN集群中，有哪些Resourcemanager节点 -->
            <property>
                <name>yarn.resourcemanager.ha.rm-ids</name>
                <value>rm1,rm3</value>
            </property>
        
            <!-- ========== RM1 的配置 ========== -->
            <!-- 指定 RM1 的主机名，即指定RM1在哪个节点上 -->
            <property>
                <name>yarn.resourcemanager.hostname.rm1</name>
                <value>hadoop132</value>
            </property>
        
            <!-- 指定 RM1 的 web 端地址 -->
            <property>
                <name>yarn.resourcemanager.webapp.address.rm1</name>
                <value>hadoop132:8088</value>
            </property>
        
            <!-- 指定 RM1 的内部通信地址 -->
            <property>
                <name>yarn.resourcemanager.address.rm1</name>
                <value>hadoop132:8032</value>
            </property>
        
            <!-- 指定 ApplicationMaster 向 RM1 申请资源的地址 -->
            <property>
                <name>yarn.resourcemanager.scheduler.address.rm1</name>
                <value>hadoop132:8030</value>
            </property>
        
            <!-- 指定供 NodeManager 连接ResourceManager的地址 -->
            <property>
                <name>yarn.resourcemanager.resource-tracker.address.rm1</name>
                <value>hadoop132:8031</value>
            </property>
        
            <!-- ========== RM3 的配置 ========== -->
            <!-- 指定 RM3 的主机名 -->
            <property>
                <name>yarn.resourcemanager.hostname.rm3</name>
                <value>hadoop134</value>
            </property>
        
            <!-- 指定 RM3 的 web 端地址 -->
            <property>
                <name>yarn.resourcemanager.webapp.address.rm3</name>
                <value>hadoop134:8088</value>
            </property>
        
            <!-- 指定 RM3 的内部通信地址 -->
            <property>
                <name>yarn.resourcemanager.address.rm3</name>
                <value>hadoop134:8032</value>
            </property>
        
            <!-- 指定 AM 向 RM3 申请资源的地址 -->
            <property>
                <name>yarn.resourcemanager.scheduler.address.rm3</name>
                <value>hadoop134:8030</value>
            </property>
        
            <!-- 指定供 NodeManager 连接的地址 -->
            <property>
                <name>yarn.resourcemanager.resource-tracker.address.rm3</name>
                <value>hadoop134:8031</value>
            </property>
        
            <!-- 指定 zookeeper 集群的地址 -->
            <property>
                <name>yarn.resourcemanager.zk-address</name>
                <value>hadoop132:2181,hadoop133:2181,hadoop134:2181</value>
            </property>
        
            <!-- 启用自动恢复 -->
            <property>
                <name>yarn.resourcemanager.recovery.enabled</name>
                <value>true</value>
            </property>
        
            <!-- 指定 ResourceManager 的状态信息存储在 zookeeper 集群 -->
            <property>
                <name>yarn.resourcemanager.store.class</name>
                <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
            </property>
        
            <!-- 环境变量的继承 -->
            <property>
                <name>yarn.nodemanager.env-whitelist</name>
                <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
            </property>
        </configuration>
        ```

    -   **配置mapred-site.xml，`vim /opt/module/hadoop-3.1.3-ha/etc/hadoop/mapred-site.xml`**

        -   **该配置项不影响HDFS HA和YARN HA的启动，但是为了部署的Hadoop高可用集群能够进行数据计算，需要配置此项目**

        ```xml
        <!-- mapred-site.xml文件配置 -->
        <configuration>
            <!-- 指定MapReduce程序运行在Yarn上 -->
            <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
            </property>
        </configuration>
        ```

    -   **配置workers，`vim /opt/module/hadoop-3.1.3-ha/etc/hadoop/workers`**

        -   **该配置文件用于配置工作节点，即指定哪些节点需要启动DataNode服务和NodeManager服务**

        ```txt
        hadoop132
        hadoop133
        hadoop134
        ```

-   **分发配置好的配置文件，`xsync /opt/module/hadoop-3.1.3-ha/etc/hadoop/`**

****

>   **==至此，所需要Zookeeper集群和配置文件均已经准备好了，下面将进行HDFS HA集群初始化启动和YARN HA集群启动==**
>
>   **==首先初始化并启动HDFS HA集群，再启动YARN HA集群==**

****

# 七、Hadoop HA集群首次启动

## 1、HDFS HA集群初始化启动

-   **编写群起群停 JournalNode服务的脚本。该脚本只会在初始化启动时会用到，后续将不会用到：`vim /home/justlancer/bin/jn_mine.sh`**

    ```bash
    #!/bin/bash
    # 群起、群停JournalNode
    # JournalNode节点只需要在hadoop132和hadoop133节点上启动即可
    if [ $# -lt 1 ]; then
        echo "缺少必要的参数"
        exit ;
    fi
    
    case $1 in
        "start" )
                for machineIP in hadoop132 hadoop133; do
                    echo "=========== 启动 $machineIP JournalNode ==========="
                    ssh $machineIP "hadoop-daemon.sh start journalnode"
                done
            ;;
        "stop" )
                for machineIP in hadoop132 hadoop133; do
                    echo "=========== 停止 $machineIP JournalNode ==========="
                    ssh $machineIP "hadoop-daemon.sh stop journalnode"
                done
            ;;
        * )
            echo '不是有效参数。参数仅包含<start><stop>'
    esac
    ```

-   **赋予脚本执行权限：`chmod u=rwx /home/justlancer/bin/jn_mine.sh`**

-   **虽然只会用一次，但是还是分发脚本，使各个节点保持一致：`xsync /home/justlancer/bin/jn_mine.sh`**

-   **HDFS HA初始化启动步骤：<u>有些步骤能够调换顺序，有些步骤不可以调换顺序。为了避免不必要的初始化错误，建议不要做修改</u>**

    -   **启动Zookeeper集群：`zk_mine.sh start`**

    -   **启动 JournalNode集群：`jn_mine.sh start`**

    -   **在任意一台节点上执行命令初始化NameNode元数据信息，以hadoop132为例：`hdfs namenode -format`**

    -   **在hadoop132节点上，单点启动NameNode服务。在哪台节点初始化的NameNode服务，就在哪台节点上启动NameNode服务：`hdfs --daemon start namenode`**

    -   **在其他部署了NameNode的节点上，执行命令，将已进行初始化操作节点的NameNode元数据信息同步过来。根据Hadoop HA集群规划，NameNode部署在hadoop132、hadoop133节点上，因此需要在hadoop133节点上执行同步NameNode元数据信息的命令：`hdfs namenode -bootstrapStandby`**

    -   **停止JournalNode集群：`jn_mine.sh stop`**

    -   **单点停止hadoop132节点的NameNode服务：`hdfs --daemon stop namenode`**

    -   **在Zookeeper中初始化HDFS HA的信息：`hdfs zkfc -formatZK`**

    -   **执行脚本启动HDFS HA集群：`start-dfs.sh`。`start-dfs.sh`脚本将会依次启动以下服务：**

        ```bash
        [justlancer@hadoop132 ~]$ start-dfs.sh 
        Starting namenodes on [hadoop132 hadoop133]
        Starting datanodes
        hadoop134: WARNING: /opt/module/hadoop-3.1.3-ha/logs does not exist. Creating.
        Starting journal nodes [hadoop132 hadoop133]
        Starting ZK Failover Controllers on NN hosts [hadoop132 hadoop133]
        ```

    -   **检查各节点运行服务，此时各节点应该运行的服务有：**

        ```txt
        ============== hadoop132 =================
        2769 NameNode
        3399 Jps
        3289 DFSZKFailoverController
        2891 DataNode
        3117 JournalNode
        1726 QuorumPeerMain
        ============== hadoop133 =================
        1840 NameNode
        1922 DataNode
        1380 QuorumPeerMain
        2020 JournalNode
        2197 Jps
        2107 DFSZKFailoverController
        ============== hadoop134 =================
        1376 QuorumPeerMain
        1593 DataNode
        1663 Jps
        ```

    -   **检查HA故障自动转移：使用kill命令停止active的NameNode服务，在网页端查看各节点NameNode服务的状态**

## 2、YARN HA集群的初始化启动

**不同于HDFS HA集群的初始化启动，YARN HA集群的初始化启动非常简单，在配置好YRAN HA集群部署文件后，直接调用脚本`start-yarn.sh`即可启动YARN HA集群。**

**`start-yarn.sh`脚本将依次启动以下服务：**

```bash
[justlancer@hadoop132 bin]$ start-yarn.sh 
Starting resourcemanagers on [ hadoop132 hadoop134]
Starting nodemanagers
```

**此时各节点应该运行的服务有：**

```txt
============== hadoop132 =================
4433 NodeManager
4310 ResourceManager
3289 DFSZKFailoverController
2891 DataNode
3564 NameNode
3117 JournalNode
1726 QuorumPeerMain
4782 Jps
============== hadoop133 =================
1922 DataNode
2562 NameNode
1380 QuorumPeerMain
2020 JournalNode
3017 Jps
2107 DFSZKFailoverController
2911 NodeManager
============== hadoop134 =================
1376 QuorumPeerMain
1921 ResourceManager
2001 NodeManager
1593 DataNode
2122 Jps
```

**进行自动故障转移检测，同样使用kill命令**

## 3、HDFS HA集群初始化启动失败重试处理

**HDFS HA集群的初始化启动由于步骤较多，并且有些步骤有固定顺序，因此很容易出现错误，导致集群初始化启动失败。根据出错的步骤能够有不同的处理办法。**

**现在给出回到最初状态，从头开始初始化启动的处理步骤：**

-   **删除三台节点hadoop-ha目录下的ha-data和logs文件：`rm -rf /opt/module/hadoop-3.1.3-ha/ha-data/ /opt/module/hadoop-3.1.3-ha/logs/`**
-   **删除Zookeeper中的hadoop-ha节点**
    -   **启动Zookeeper集群：`zk_mine.sh start`**
    -   **连接Zookeeper服务：`zkCli.sh -server hadoop132:2181`**
    -   **删除hadoop-ha节点：`deleteall /hadoop-ha`**
    -   **退出：`quit`**

**完成以上两个步骤之后，就可以从头开始进行HDFS HA集群的初始化启动了。**

# 八、安装并配置Kafka集群

**Kafka的安装和配置较为简单，只需要上传解压安装包，并做简单配置即可。**

**==以下操作默认在hadoop132节点上==**

-   **利用xftp工具将`Kafka-2.12-3.0.0.tgz`上传到`/opt/software`目录下**

-   **将Kafka安装包解压到`/opt/module`目录下：`tar -zxvf /opt/software/kafka_2.12-3.0.0.tgz -C /opt/module/`**

-   **配置环境变量：`vim /etc/profile.d/my_env.sh`**

    ```txt
    # KAFKA_HOME
    export KAFKA_HOME=/opt/module/kafka_2.12-3.0.0
    export PATH=$PATH:$KAFKA_HOME/bin
    ```

-   **执行文件，让环境变量生效：`source /etc/profile`**

-   **分发环境变量文件，并在各节点中都执行文件，使环境变量生效：`sudo /home/justlancer/bin/xsync  /etc/profile.d/my_env.sh`**

-   **修改Kafka服务端配置文件：`vim /opt/module/kafka_2.12-3.0.0/config/server.properties`**![image-20230306111410608](.\大数据组件部署文档\image-20230306111410608.png)![image-20230306111559121](.\大数据组件部署文档\image-20230306111559121.png)![image-20230306111713410](.\大数据组件部署文档\image-20230306111713410.png)

-   **分发配置文件到hadoop133、hadoop134节点：`xsync /opt/module/kafka_2.12-3.0.0/config/server.properties`**

-   **修改hadoop133、hadoop134节点`server.properties`的`broker.id`配置项**

-   **编写Kafka集群群起群停的脚本：`vim /home/justlancer/bin/kf_mine.sh`**

    ```bash
    #! /bin/bash
    if [ $# -lt 1 ]; then
        echo '脚本缺少参数'
        exit;
    fi
    
    case $1 in
    "start"){
        for i in hadoop132 hadoop133 hadoop134
        do
            echo " --------启动 $i Kafka-------"
            ssh $i "/opt/module/kafka_2.12-3.0.0/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.12-3.0.0/config/server.properties"
        done
    };;
    "stop"){
        for i in hadoop132 hadoop133 hadoop134
        do
            echo " --------停止 $i Kafka-------"
            ssh $i "/opt/module/kafka_2.12-3.0.0/bin/kafka-server-stop.sh "
        done
    };;
    * ){
        echo '不是有效参数。参数仅包含<start><stop>'
    }
    esac
    ```

-   **赋予脚本执行权限：`chmod u=rwx /home/justlancer/bin/kf_mine.sh`**

-   **分发脚本：`xsync /home/justlancer/bin/kf_mine.sh`**

**==至此，Kafka集群就已经安装完成。==**

**==启动Kafka集群时，必须先启动Zookeeper集群；关闭Kafka集群时，必须先等Kafka集群全部节点都关闭，才能关闭Zookeeper集群。==**
